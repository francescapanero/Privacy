# Norms
frobenius.norm(A-A_svd_k)^2 # euqual to sum(svd_A$d^2) - sum(svd_A$d[1:k]^2)
spectral.norm(A-A_svd_k) # equal to sum(svd_A$d[k+1])
B = A
merged1 <- c(2,3,4)
merged2 <- c(10,11,12)
merged3 <- c(17,18,19)
B = A
B[,merged1[1]] =  sum(B[,merged1])
B = A
B[,merged1[1]] =  sum(B[,merged1])
B[merged1[1],] =  sum(B[merged1,])
B = B[-merged1[2:length(merged1)],-merged1[2:length(merged1)]]
B[,merged2[1]] =  sum(B[,merged2])
B[merged2[1],] =  sum(B[merged2,])
B = B[-merged2[2:length(merged2)],-merged1[2:length(merged2)]]
B[,merged3[1]] =  sum(B[,merged3])
B[merged3[1],] =  sum(B[merged3,])
B = B[-merged2[2:length(merged3)],-merged1[2:length(merged3)]]
B = A
B[,merged1[1]] =  sum(B[,merged1])
B[merged1[1],] =  sum(B[merged1,])
B = B[-merged1[2:length(merged1)],-merged1[2:length(merged1)]]
B[,merged2[1]] =  sum(B[,merged2])
B[merged2[1],] =  sum(B[merged2,])
B = B[-merged2[2:length(merged2)],-merged1[2:length(merged2)]]
B[,merged3[1]] =  sum(B[,merged3])
B[merged3[1],] =  sum(B[merged3,])
B = A
B[,merged1[1]] =  sum(B[,merged1])
B[merged1[1],] =  sum(B[merged1,])
B[,merged2[1]] =  sum(B[,merged2])
B[merged2[1],] =  sum(B[merged2,])
B[,merged3[1]] =  sum(B[,merged3])
B[merged3[1],] =  sum(B[merged3,])
B = B[-merged1[2:length(merged1)],-merged1[2:length(merged1)]]
B = B[-merged2[2:length(merged2)],-merged1[2:length(merged2)]]
B = B[-merged3[2:length(merged3)],-merged1[2:length(merged3)]]
diag(B) = rep(0, nrow(B))
merged1 <- c(2,3,4)
merged2 <- c(10,11,12)
merged3 <- c(17,18,19)
B = A
B[,merged1[1]] =  sum(B[,merged1])
B[merged1[1],] =  sum(B[merged1,])
B[,merged2[1]] =  sum(B[,merged2])
B[merged2[1],] =  sum(B[merged2,])
B[,merged3[1]] =  sum(B[,merged3])
B[merged3[1],] =  sum(B[merged3,])
B = B[-merged1[2:length(merged1)],-merged1[2:length(merged1)]]
B = B[-merged2[2:length(merged2)],-merged1[2:length(merged2)]]
B = A
B[,merged1[1]] =  sum(B[,merged1])
B[merged1[1],] =  sum(B[merged1,])
B[,merged2[1]] =  sum(B[,merged2])
B[merged2[1],] =  sum(B[merged2,])
B[,merged3[1]] =  sum(B[,merged3])
B[merged3[1],] =  sum(B[merged3,])
B = B[-c(merged1[2:length(merged1),merged2[2:length(merged2),merged3[2:length(merged3)),
B = A
B[,merged1[1]] =  sum(B[,merged1])
B[merged1[1],] =  sum(B[merged1,])
B[,merged2[1]] =  sum(B[,merged2])
B[merged2[1],] =  sum(B[merged2,])
B[,merged3[1]] =  sum(B[,merged3])
B[merged3[1],] =  sum(B[merged3,])
B = B[-c(merged1[2:length(merged1)],merged2[2:length(merged2)],merged3[2:length(merged3)]),
-c(merged1[2:length(merged1)],merged2[2:length(merged2)],merged3[2:length(merged3)])]
diag(B) = rep(0, nrow(B))
frobenius.norm(A-A_svd_k)^2 # euqual to sum(svd_A$d^2) - sum(svd_A$d[1:k]^2)
spectral.norm(A-A_svd_k)
library(e1071)
library(ROCR)
library(ggplot2)
library(MASS)
# Load the MASS package and combine Pima.tr and Pima.tr2 to a data.frame
# called train and save Pima.te as test. Change the coding of our variable of
# interest to (type) to 0 (non-diabetic) and 1 (diabetic). Check for and take note of any missing values.
train <- data.frame(rbind(Pima.tr, Pima.tr2))
test <- data.frame(Pima.te)
sum(is.na(test$type))
sum(is.na(train$type))
logistic <- glm(type ~ age, data = train, family = binomial)
summary(logistic)
glm.probs <- predict(logistic,test,type = "response")
glm.probs[1:5]
glm.pred <- ifelse(glm.probs > 0.5, 1, 0)
table(glm.pred,test$type)
mean(glm.pred == test$type)
library(ROCR)
perf <- performance(glm.pred,"tpr","fpr")
pred <- prediction(glm.pred, test$type)
perf <- performance(pred,"tpr","fpr")
plot(perf,colorize=TRUE)
lift.roc<- function(pred, g, type="bin", plot.it=TRUE)
{
library(sm)
if(plot.it) {par(mfrow=c(1,2))}
if(!is.numeric(g)) stop("g not numeric")
ind <- rev(order(pred))
n <- length(g)
x1 <-  (1:n)/n
x2 <- cumsum(g[ind])/(mean(g)*(1:n))
if(type=="crude" & plot.it)
plot(x1, x2, type="l", col=2,
xlab="fraction", ylab="lift")
if(type=="sm") {
a<- sm.regression(x1, x2, h=0.1, display="none")
if(plot.it)
plot(a$eval, a$estimate, type="l",xlim=c(0,1), col=2,
xlab="fraction", ylab="lift")
}
if(type=="bin") {
b <-  binning(x1,x2, breaks=(-0.001:10)/9.999)
x <- c(0,seq(0.05,0.95, by=0.1),1)
if(plot.it) plot(x, c(x2[1],b$means,1), type="b", xlim=c(0,1),
ylim=c(1,max(x2)), cex=0.75, col=2,
xlab="fraction",
ylab="lift")
x1<- x
x2<- c(x2[1],b$means,1)
}
u1<- cumsum(1-g[ind])/sum(1-g)
u2<- cumsum(g[ind])/sum(g)
if(type=="crude" & plot.it)
plot(u1, u2, type="l", xlim=c(0,1), ylim=c(0,1), col=2,
xlab="1-specificity", ylab="sensitivity")
if(type=="sm") {
# browser()
eps<- 0.00001
a<- sm.regression(u1,log((u2+eps)/(1-u2+2*eps)), h=0.1, display="none")
q<- exp(a$estimate)/(1+exp(a$estimate))
if(plot.it) plot(a$eval, q, type="l", xlim=c(0,1), ylim=c(0,1),
xlab="1-specificity", ylab="sensitivity", col=2)
}
if(type=="bin") {
b <- binning(u1,u2, breaks=(-0.001:10)/9.999)
x <- c(0,seq(0.05,0.95, by=0.1),1)
y<- c(0,b$means,1)
if(plot.it)
plot(x, y, type="b", xlim=c(0,1),
ylim=c(0,1),cex=0.75, xlab="1-specificity",
ylab="sensitivity", col=2)
u1<- x
u2<- y
}
if(plot.it) {
abline(0,1, lty=2, col=3)
}
invisible(cbind(x1,x2,u1,u2))
}
confusion.mat <- function(pred, observed){
n <-  table(pred,observed)
err.tot <- 1-sum(diag(n))/sum(n)
fn <- n[1,2]/(n[1,1]+n[1,2])
fp <- n[2,1]/(n[2,1]+n[2,2])
print(n)
cat("global error: ", format(err.tot),"\n")
cat("FP & FN: ",format(c(fp, fn)),"\n")
invisible(n)
}
lift.roc(glm.pred, test$type)
install.packages("sm")
lift.roc(glm.pred, test$type)
test$type = (test$type=="Yes")
test$type[test$type==FALSE] <- 0
test$type[test$type==TRUE] <- 1
lift.roc(glm.pred, test$type)
logistic <- glm(type ~ ., data = train, family = binomial)
summary(logistic)
mod1 <- glm(type ~ ., data = train, family = binomial)
summary(mod1)
glm.probs <- predict(mod1,test,type = "response")
glm.probs1 <- predict(mod1,test,type = "response")
mod2 <- glm(type ~ npreg+glu+bmi+ped+age, data = train, family = binomial)
summary(mod2)
glm.probs2 <- predict(mod2,test,type = "response")
lift.roc(glm.probs1, test$type)
lift.roc(glm.probs2, test$type)
mod3 = randomForest(type ~ ., data=train, ntree=100, mtry=2, importance=TRUE)
install.packages("randomForest")
library(randomForest)
mod3 = randomForest(type ~ ., data=train, ntree=100, mtry=2, importance=TRUE)
train$type = (train$type=="Yes")
train$type[train$type==FALSE] <- 0
train$type[train$type==TRUE] <- 1
mod3 = randomForest(type ~ ., data=train, ntree=100, mtry=2, importance=TRUE)
train$type
sum(is.na(train))
?randomForest
mod3 = randomForest(type ~ ., data=train, ntree=100, mtry=2, importance=TRUE, na.action=na.omit)
complete.cases(train)
sum(complete.cases(train))
sum(complete.cases(train))
mod3 = randomForest(type ~ ., data=train, ntree=100, mtry=2, importance=TRUE, na.action=na.omit)
mod3 = randomForest(type ~ ., data=train, ntree=100, importance=TRUE, na.action=na.omit)
train$type
mod3 = randomForest(as.factor(type) ~ ., data=train, ntree=100, importance=TRUE, na.action=na.omit)
summary(mod3)
mod3
mod4 = randomForest(as.factor(type) ~ ., data=train, ntree=300, importance=TRUE, na.action=na.omit)
mod4
varImpPlot(mod4)
predict(mod4, test)
pred4 <- predict(mod4, test)
probs4 <- predict(mod4, test, type="prob")
lift.roc(glm.probs4, test$type)
lift.roc(probs4, test$type)
probs4 <- predict(mod4, test, type="prob")
probs4
probs4 <- max(predict(mod4, test, type="prob"),1)
probs4 <- max.row(predict(mod4, test, type="prob"))
?apply
probs4 <- apply(max, 1, predict(mod4, test, type="prob"))
prob <- predict(mod4, test, type="prob")
probs4 <- apply(max, 1, prob)
probs4 <- apply(prob, 1, max)
lift.roc(probs4, test$type)
probs4
dim(test)
as.numeric(probs4)
probs4 <- as.numeric(apply(prob, 1, max))
lift.roc(glm.probs1, test$type)
lift.roc(glm.probs2, test$type)
lift.roc(probs4, test$type)
probs4 <- prob[,1]
lift.roc(glm.probs1, test$type)
lift.roc(glm.probs2, test$type)
lift.roc(probs4, test$type)
probs4 <- prob[,2]
lift.roc(glm.probs1, test$type)
lift.roc(glm.probs2, test$type)
lift.roc(probs4, test$type)
mod4
varImpPlot(mod4)
mod5 = randomForest(as.factor(type) ~ glu+ped+age+bmi, data=train, ntree=300, importance=TRUE, na.action=na.omit)
mod5
prob <- predict(mod4, test, type="prob")
prob <- predict(mod5, test, type="prob")
probs4 <- prob[,2]
lift.roc(glm.probs1, test$type)
lift.roc(glm.probs2, test$type)
lift.roc(probs4, test$type)
mod6 <- svm(type ~ ., data = train, kernel = "linear", cost = 10, scale = FALSE)
print(svmfit)
mod6 <- svm(type ~ ., data = train, kernel = "linear", cost = 5, scale = FALSE)
print(mod6)
pred <- predict(mod6, test)
head(pprecip)
head(pred)
pred <- predict(mod6, test, type = "prob")
head(pred)
head(test[,-1])
head(test[,1:7])
pred <- predict(mod6, test[,1:7], type = "prob")
head(pred)
print(mod6)
pred <- predict(mod6, test[,1:7])
head(pred)
mod6 <- svm(type ~ ., data = train, kernel = "circular", cost = 5, scale = FALSE)
linear
mod6 <- svm(type ~ ., data = train, kernel = "linear", cost = 5, scale = FALSE)
print(mod6)
pred <- predict(mod6, test[,1:7])
pred
?predict
set.seed(10111)
x = matrix(rnorm(40), 20, 2)
y = rep(c(-1, 1), c(10, 10))
x[y == 1,] = x[y == 1,] + 1
plot(x, col = y + 3, pch = 19)
dat = data.frame(x, y = as.factor(y))
svmfit = svm(y ~ ., data = dat, kernel = "linear", cost = 10, scale = FALSE)
print(svmfit)
make.grid = function(x, n = 75) {
grange = apply(x, 2, range)
x1 = seq(from = grange[1,1], to = grange[2,1], length = n)
x2 = seq(from = grange[1,2], to = grange[2,2], length = n)
expand.grid(X1 = x1, X2 = x2)
}
make.grid = function(x, n = 75) {
grange = apply(x, 2, range)
x1 = seq(from = grange[1,1], to = grange[2,1], length = n)
x2 = seq(from = grange[1,2], to = grange[2,2], length = n)
expand.grid(X1 = x1, X2 = x2)
}
xgrid = make.grid(x)
xgrid[1:10,]
ygrid = predict(svmfit, xgrid)
plot(xgrid, col = c("red","blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
points(x[svmfit$index,], pch = 5, cex = 2)
beta = drop(t(svmfit$coefs)%*%x[svmfit$index,])
beta0 = svmfit$rho
plot(xgrid, col = c("red", "blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
points(x[svmfit$index,], pch = 5, cex = 2)
abline(beta0 / beta[2], -beta[1] / beta[2])
abline((beta0 - 1) / beta[2], -beta[1] / beta[2], lty = 2)
abline((beta0 + 1) / beta[2], -beta[1] / beta[2], lty = 2)
ygrid
?svm
mod6 <- svm(type ~ ., data = train, kernel = "linear", cost = 5, scale = FALSE, probability = TRUE)
print(mod6)
pred <- predict(mod6, test[,1:7])
pred <- predict(mod6, test)
pred
pred <- predict(mod6, test, probability = T)
pred
pred <- predict(mod6, test, probability = TRUE)
head(attr(pred, "probabilities"))
mod6 <- svm(as.factor(type) ~ ., data = train, kernel = "linear", cost = 5, scale = FALSE, probability = TRUE)
print(mod6)
pred <- predict(mod6, test, probability = TRUE)
head(attr(pred, "probabilities"))
pred
lift.roc(pred[,1], test$type)
lift.roc(pred$probabilities[,1], test$type)
pred$probabilities
lift.roc(attr(pred, "probabilities")[,1], test$type)
lift.roc(attr(pred, "probabilities")[,0], test$type)
attr(pred, "probabilities")[,0]
lift.roc(attr(pred, "probabilities")[,2], test$type)
lift.roc(glm.probs1, test$type)
lift.roc(glm.probs2, test$type)
lift.roc(probs4, test$type)
lift.roc(attr(pred, "probabilities")[,2], test$type)
install.packages("OptimalCutpoints")
library(optimalCutpoints)
library(OptimalCutpoints)
?youden
??youden
install.packages("pROC")
library(pROC)
plot.roc(glm.probs1, test$type)
?plot.roc
data(aSAH)
roc.s100b <- roc(aSAH$outcome, aSAH$s100b)
roc(aSAH$outcome, aSAH$s100b)
plot(roc.s100b)
par(mfrow=c(1,1))
plot(roc.s100b)
aSAH$outcome
aSAH$s100b
roc <- roc(test$type, glm.probs1)
plot.roc(roc)
lift.roc(glm.probs1, test$type)
plot.roc(roc)
par(mfrow=c(1,1))
ROC <- plot.roc(roc)
coords(ROC, "b", ret="t", best.method="youden")
?coords
coords(roc, "b", ret="t", best.method="youden")
coords(roc, "all", ret="t", best.method="youden")
coords(roc, "all", transpose = FALSE, best.method="youden")
coords(roc, "b", ret = "t", best.method = "closest.topleft")
ROC <- plot.roc(roc)
plot(roc, print.thres = "best", print.thres.best.method = "youden")
plot(roc, print.thres = "best", print.thres.best.method = "closest.topleft")
roc$thresholds[which.max(roc$sensitivities + roc$specificities)]
# QUESTO PER TROVARE THRESHOLD
t1 <- roc$thresholds[which.max(roc$sensitivities + roc$specificities)]
confusion.mat(glm.probs1>t1, test$type)
roc2 <- roc(test$type, glm.probs2)
t2 <- roc2$thresholds[which.max(roc2$sensitivities + roc2$specificities)]
confusion.mat(glm.probs2>t2, test$type)
confusion.mat(glm.probs2>t2, test$type)
confusion.mat(glm.probs2<t2, test$type)
confusion.mat(glm.probs2>t2, test$type)
confusion.mat(glm.probs1>t1, test$type)
roc3 <- roc(test$type, probs4)
t4 <- roc4$thresholds[which.max(roc4$sensitivities + roc4$specificities)]
roc3 <- roc(test$type, probs4)
t3 <- roc3$thresholds[which.max(roc3$sensitivities + roc3$specificities)]
confusion.mat(probs4>t3, test$type)
roc4 <- roc(test$type, pred)
t4 <- roc4$thresholds[which.max(roc4$sensitivities + roc4$specificities)]
roc4 <- roc(test$type, attr(pred, "probabilities"))
attr(pred, "probabilities")
roc4 <- roc(test$type, attr(pred, "probabilities")[,2])
t4 <- roc4$thresholds[which.max(roc4$sensitivities + roc4$specificities)]
confusion.mat(attr(pred, "probabilities")[,2]>t4, test$type)
library(e1071)
library(ROCR)
library(ggplot2)
library(randomForest)
library(OptimalCutpoints)
library(pROC)
rep(0,3)
!is.numeric('a')
is.numeric('a')
10>10e9
10e9
1e9
mat <- matrix(c(22.2,  25.5,
23.1 , 25.9,
23.4,  26.1), ncol=2, byrow=TRUE)
trueval <- 23.3
apply(mat, 1, findInterval, x=trueval)
?findInterval
mat <- matrix(c(22.2,  25.5,
23.1 , 25.9,
23.4,  26.1), ncol=2, byrow=TRUE)
trueval <- 23.4
apply(mat, 1, findInterval, x=trueval)
mat <- matrix(c(22.2,  25.5,
23.1 , 25.9,
23.4,  26.1), ncol=2, byrow=TRUE)
trueval <- 26.1
apply(mat, 1, findInterval, x=trueval)
mat <- matrix(c(22.2,  25.5,
23.1 , 25.9,
23.4,  26.1), ncol=2, byrow=TRUE)
trueval <- 26.1
apply(mat, 1, findInterval, x=trueval)
mat <- matrix(c(22.2,  25.5,
+                 23.1 , 25.9,
+                 23.4,  26.1), ncol=2, byrow=TRUE)
trueval <- 26.1
apply(mat, 1, findInterval, x=trueval)
between(ymd("2014-01-01"), "2013-01-01", "2015-01-01")
library(dplyr)
between(ymd("2014-01-01"), "2013-01-01", "2015-01-01")
install.packages("lubridate")
library(lubridate)
?ts
temp=c(34.38
,34.36
,34.74
,35.26
,35.23
,35.29
,35.64
,36.02
,36.1
,36.98
,37.01
,36.75
,36.01
,35.66
,34.72
,33.9
,32.62
,31.51
,30.73
,29.5
,26.94
,25.47
,23.84
,22.55)
temp <- ts(temp, frequency=24, start=c("2013-01-01",1)),
ts.plot(temp)
temp <- ts(temp, frequency=24, start=c("2013-01-01",1))
ts.plot(temp)
temp <- ts(temp, frequency=24)
ts.plot(temp)
install.packages("forecast")
library(forecast)
forc <- HoltWinters(temp, beta=FALSE, gamma=FALSE)
forc2 <- forecast.HoltWinters(forc, h=8)
#install.packages("forecast")
library(forecast)
forc2 <- forecast.HoltWinters(forc, h=8)
length(temp)
s(1:2)
?gam
??gam
forc <- HoltWinters(temp)
temp <- ts(temp, frequency=24)
ts.plot(temp)
forc <- HoltWinters(temp)
forc <- HoltWinters(temp, beta=FALSE, gamma=FALSE)
forc2 <- forecast(forc, h=24)
ts.plot(forc2)
forc2
forc2$forecast
?HoltWinters
forc <- HoltWinters(temp, gamma=FALSE)
forc2 <- forecast(forc, h=24)
ts.plot(forc2)
forc2
forc2[,1]
?forecast
forc2
summary(forc2)
forc2$Forecasts
forc2$fitted
ts
temp
forc2$fitted
forc <- HoltWinters(temp, gamma=FALSE)
forc2 <- forecast(forc, h=24)
ts.plot(forc2)
plot(forc2$fitted)
ts.plot(temp)
forc2$fitted
na.rm(forc2$fitted)
c(forc2$fitted, na.rm=T)
na.omit(forc2$fitted)
as.numeric(na.omit(forc2$fitted))
?lm
?colnames
setwd("~/Documents/Privacy_git/Privacy")
library(tidyverse)
library(dplyr)
library(knitr)
library(poweRlaw)
rm(list=ls())
source("functions.R")
cal1  <- read.table("data/campione_cal_1.txt", header=FALSE)
prova2 <- read.csv("data/usa_00002.csv", header = T)
?sampling
