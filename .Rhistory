23.1 , 25.9,
23.4,  26.1), ncol=2, byrow=TRUE)
trueval <- 26.1
apply(mat, 1, findInterval, x=trueval)
mat <- matrix(c(22.2,  25.5,
23.1 , 25.9,
23.4,  26.1), ncol=2, byrow=TRUE)
trueval <- 26.1
apply(mat, 1, findInterval, x=trueval)
mat <- matrix(c(22.2,  25.5,
+                 23.1 , 25.9,
+                 23.4,  26.1), ncol=2, byrow=TRUE)
trueval <- 26.1
apply(mat, 1, findInterval, x=trueval)
between(ymd("2014-01-01"), "2013-01-01", "2015-01-01")
library(dplyr)
between(ymd("2014-01-01"), "2013-01-01", "2015-01-01")
install.packages("lubridate")
library(lubridate)
?ts
temp=c(34.38
,34.36
,34.74
,35.26
,35.23
,35.29
,35.64
,36.02
,36.1
,36.98
,37.01
,36.75
,36.01
,35.66
,34.72
,33.9
,32.62
,31.51
,30.73
,29.5
,26.94
,25.47
,23.84
,22.55)
temp <- ts(temp, frequency=24, start=c("2013-01-01",1)),
ts.plot(temp)
temp <- ts(temp, frequency=24, start=c("2013-01-01",1))
ts.plot(temp)
temp <- ts(temp, frequency=24)
ts.plot(temp)
install.packages("forecast")
library(forecast)
forc <- HoltWinters(temp, beta=FALSE, gamma=FALSE)
forc2 <- forecast.HoltWinters(forc, h=8)
#install.packages("forecast")
library(forecast)
forc2 <- forecast.HoltWinters(forc, h=8)
length(temp)
s(1:2)
?gam
??gam
forc <- HoltWinters(temp)
temp <- ts(temp, frequency=24)
ts.plot(temp)
forc <- HoltWinters(temp)
forc <- HoltWinters(temp, beta=FALSE, gamma=FALSE)
forc2 <- forecast(forc, h=24)
ts.plot(forc2)
forc2
forc2$forecast
?HoltWinters
forc <- HoltWinters(temp, gamma=FALSE)
forc2 <- forecast(forc, h=24)
ts.plot(forc2)
forc2
forc2[,1]
?forecast
forc2
summary(forc2)
forc2$Forecasts
forc2$fitted
ts
temp
forc2$fitted
forc <- HoltWinters(temp, gamma=FALSE)
forc2 <- forecast(forc, h=24)
ts.plot(forc2)
plot(forc2$fitted)
ts.plot(temp)
forc2$fitted
na.rm(forc2$fitted)
c(forc2$fitted, na.rm=T)
na.omit(forc2$fitted)
as.numeric(na.omit(forc2$fitted))
?lm
?colnames
setwd("~/Documents/Privacy_git/Privacy")
library(tidyverse)
library(dplyr)
library(knitr)
library(poweRlaw)
rm(list=ls())
source("functions.R")
prova2 <- read.csv("data/usa_00002.csv", header = T)
head(prova2)
prova2sub <-prova2[prova2$AGE > 20,]
prova2sub <- prova2sub %>% dplyr::mutate(id = 1:n())
id_uniq <- read.table("data/id_uniq.txt", header=TRUE)
id_uniq <- id_uniq$id
id_uniq <- read.table("data/id_uniq.txt", header=TRUE)
freq_city <- read.table("data/freq_city1.txt", header=TRUE)
freq_city <- freq_city$count
id_uniq <- read.table("data/id_uniq.txt", header=TRUE)
id_uniq <- id_uniq$id
tablestate <- prova2sub %>% group_by(STATEICP, HHINCOME, VALUEH, FAMSIZE, NCHILD, SEX, AGE, MARST, RACE,
HCOVANY, EDUC, SCHLTYPE, EMPSTAT, OCC,
INCTOT, FTOTINC, VETSTAT) %>% summarize(count=n())
freq_state <- tablestate$count
m_pl = displ$new(freq_state)
est = estimate_xmin(m_pl)
m_pl$setXmin(est)
plot(m_pl, main="US 2018 state")
lines(m_pl, col=2, main="US 2018 state")
tablestate <- prova2sub %>% group_by(STATEICP, HHINCOME, VALUEH, FAMSIZE, NCHILD, SEX, AGE, MARST, RACE,
HCOVANY, EDUC, SCHLTYPE, EMPSTAT, OCC,
INCTOT) %>% summarize(count=n())
freq_state <- tablestate$count
m_pl = displ$new(freq_state)
est = estimate_xmin(m_pl)
m_pl$setXmin(est)
plot(m_pl, main="US 2018 state")
lines(m_pl, col=2, main="US 2018 state")
N = dim(prova2sub)[1]
percentage0 = 0.1
n0 = as.integer(percentage0*N)
ind0 <- sample(1:N, n0, replace = FALSE)
sample_n0 <- prova2sub[ind0,]
id_uniqsamp1_state <- sample_n0 %>% add_count(STATEICP, HHINCOME, VALUEH, FAMSIZE, NCHILD, SEX, AGE, MARST, RACE,
HCOVANY, EDUC, SCHLTYPE, EMPSTAT, OCC,
INCTOT) %>%
filter(n==1) %>% select(id)
id_uniqsamp1_state <- id_uniqsamp1_state$id
table_samp_state <- sample_n0 %>% group_by(STATEICP, HHINCOME, VALUEH, FAMSIZE, NCHILD, SEX, AGE, MARST, RACE,
HCOVANY, EDUC, SCHLTYPE, EMPSTAT, OCC,
INCTOT) %>% count()
table_samp_state <- sample_n0 %>% group_by(STATEICP, HHINCOME, VALUEH, FAMSIZE, NCHILD, SEX, AGE, MARST, RACE,
HCOVANY, EDUC, SCHLTYPE, EMPSTAT, OCC,
INCTOT) %>% count()
freq_samp_state <- table_samp_state$n
id_state <- prova2sub %>% add_count(STATEICP, HHINCOME, VALUEH, FAMSIZE, NCHILD, SEX, AGE, MARST, RACE,
HCOVANY, EDUC, SCHLTYPE, EMPSTAT, OCC,
INCTOT) %>% filter(n==1) %>% select(id)
id_state <- id_state$id
# compute tau_1 true
id_alluniq_state <- intersect(id_uniqsamp1_state, id_state)
tau1_true_reg <- length(id_alluniq_reg)
tau1_true_reg <- length(id_alluniq_state)
tau1_true_state <- length(id_alluniq_state)
# comparison uniques in sample AND pop, uniques in sample, uniques in population
tau1_true_state # pop and sample uniques
length(id_state)/N # pop uniques
length(id_uniqsamp1_state)/n0 # sample uniques
# number of uniques in sample
m1_state <- sum(freq_samp_state==1)
# if you wanna plot the frequency counts
tablefreq_state <- table(freq_samp_state)
tablefreq_state <- data.frame("frequency"=as.factor(names(tablefreq_state)), "count"=as.numeric(tablefreq_state))
ggplot(data=tablefreq_state, aes(reorder(frequency, -count), log(count))) + geom_col() + ggtitle('City log frequencies counts sample 10%')
# Stime
out_PY <- max_EPPF_PY(freq_samp_reg)
# Stime
out_PY <- max_EPPF_PY(freq_samp_state)
tau1_PY <- tau1_py(m1_state, n0, out_PY$par[1], out_PY$par[2], N)
PY1_sim <- tau1_py_sim(freq_samp_state, out_PY$par[1], out_PY$par[2], N)
PY1_lower <- quantile(PY1_sim, 0.025)
PY1_upper <- quantile(PY1_sim, 0.975)
out_DP <- max_EPPF_DP(freq_samp_state)
tau1_DP <- tau1_dp(m1_state, n0, out_DP$par, N)
DP1_sim  <- tau1_py_sim(freq_samp_state, out_DP$par, 0, N)
DP1_lower <- quantile(DP1_sim, 0.025)
DP1_upper <- quantile(DP1_sim, 0.975)
# Binomial approximation
tau1_py_binom1 <- m1_state * (n0 / N)^(1 - out_PY$par[2])
lower_py_binom1 <- qbinom(0.025, m1_state, (n0 / N)^(1 - out_PY$par[2]))
upper_py_binom1 <- qbinom(0.975, m1_state, (n0 / N)^(1 - out_PY$par[2]))
kable(data.frame(n = n0, N = N, percentage = percentage0,
K_n = dim(table_n1)[1], m1 = m1_reg,
true_tau1 = tau1_true_reg,
tau1_py = tau1_PY, CI_PY = paste("[", PY1_lower,", ", PY1_upper,"]",sep=""),
tau1_py_binapprox = tau1_py_binom1,
CI_PY_binapprox = paste("[", lower_py_binom1,", ", upper_py_binom1,"]",sep=""),
tau1_dp = tau1_DP, CI_DP = paste("[", DP1_lower,", ", DP1_upper,"]",sep="")))
kable(data.frame(n = n0, N = N, percentage = percentage0,
m1 = m1_reg,
true_tau1 = tau1_true_reg,
tau1_py = tau1_PY, CI_PY = paste("[", PY1_lower,", ", PY1_upper,"]",sep=""),
tau1_py_binapprox = tau1_py_binom1,
CI_PY_binapprox = paste("[", lower_py_binom1,", ", upper_py_binom1,"]",sep=""),
tau1_dp = tau1_DP, CI_DP = paste("[", DP1_lower,", ", DP1_upper,"]",sep="")))
kable(data.frame(n = n0, N = N, percentage = percentage0,
m1 = m1_state,
true_tau1 = tau1_true_reg,
tau1_py = tau1_PY, CI_PY = paste("[", PY1_lower,", ", PY1_upper,"]",sep=""),
tau1_py_binapprox = tau1_py_binom1,
CI_PY_binapprox = paste("[", lower_py_binom1,", ", upper_py_binom1,"]",sep=""),
tau1_dp = tau1_DP, CI_DP = paste("[", DP1_lower,", ", DP1_upper,"]",sep="")))
kable(data.frame(n = n0, N = N, percentage = percentage0,
m1 = m1_state,
true_tau1 = tau1_true_state,
tau1_py = tau1_PY, CI_PY = paste("[", PY1_lower,", ", PY1_upper,"]",sep=""),
tau1_py_binapprox = tau1_py_binom1,
CI_PY_binapprox = paste("[", lower_py_binom1,", ", upper_py_binom1,"]",sep=""),
tau1_dp = tau1_DP, CI_DP = paste("[", DP1_lower,", ", DP1_upper,"]",sep="")))
tablestate <- prova2sub %>% group_by(STATEICP, HHINCOME, VALUEH, FAMSIZE, NCHILD, SEX, AGE, MARST,
HCOVANY, EDUC, SCHLTYPE, EMPSTAT, OCC,
INCTOT, FTOTINC, VETSTAT) %>% summarize(count=n())
freq_state <- tablestate$count
m_pl = displ$new(freq_state)
est = estimate_xmin(m_pl)
m_pl$setXmin(est)
plot(m_pl, main="US 2018 state")
lines(m_pl, col=2, main="US 2018 state")
id_state <- prova2sub %>% add_count(STATEICP, HHINCOME, VALUEH, FAMSIZE, NCHILD, SEX, AGE, MARST,
HCOVANY, EDUC, SCHLTYPE, EMPSTAT, OCC,
INCTOT) %>% filter(n==1) %>% select(id)
id_state <- id_state$id
N = dim(prova2sub)[1]
percentage0 = 0.1
n0 = as.integer(percentage0*N)
ind0 <- sample(1:N, n0, replace = FALSE)
sample_n0 <- prova2sub[ind0,]
id_uniqsamp1_state <- sample_n0 %>% add_count(STATEICP, HHINCOME, VALUEH, FAMSIZE, NCHILD, SEX, AGE, MARST, RACE,
HCOVANY, EDUC, SCHLTYPE, EMPSTAT, OCC,
INCTOT) %>%
filter(n==1) %>% select(id)
id_uniqsamp1_state <- id_uniqsamp1_state$id
table_samp_state <- sample_n0 %>% group_by(STATEICP, HHINCOME, VALUEH, FAMSIZE, NCHILD, SEX, AGE, MARST, RACE,
HCOVANY, EDUC, SCHLTYPE, EMPSTAT, OCC,
INCTOT) %>% count()
freq_samp_state <- table_samp_state$n
# compute tau_1 true
id_alluniq_state <- intersect(id_uniqsamp1_state, id_state)
tau1_true_state <- length(id_alluniq_state)
# comparison uniques in sample AND pop, uniques in sample, uniques in population
tau1_true_state # pop and sample uniques
length(id_state)/N # pop uniques
length(id_uniqsamp1_state)/n0 # sample uniques
# number of uniques in sample
m1_state <- sum(freq_samp_state==1)
# if you wanna plot the frequency counts
tablefreq_state <- table(freq_samp_state)
tablefreq_state <- data.frame("frequency"=as.factor(names(tablefreq_state)), "count"=as.numeric(tablefreq_state))
ggplot(data=tablefreq_state, aes(reorder(frequency, -count), log(count))) + geom_col() + ggtitle('City log frequencies counts sample 10%')
# Stime
out_PY <- max_EPPF_PY(freq_samp_state)
tau1_PY <- tau1_py(m1_state, n0, out_PY$par[1], out_PY$par[2], N)
PY1_sim <- tau1_py_sim(freq_samp_state, out_PY$par[1], out_PY$par[2], N)
PY1_lower <- quantile(PY1_sim, 0.025)
PY1_upper <- quantile(PY1_sim, 0.975)
out_DP <- max_EPPF_DP(freq_samp_state)
tau1_DP <- tau1_dp(m1_state, n0, out_DP$par, N)
DP1_sim  <- tau1_py_sim(freq_samp_state, out_DP$par, 0, N)
DP1_lower <- quantile(DP1_sim, 0.025)
DP1_upper <- quantile(DP1_sim, 0.975)
# tau1_NP_pois <- tau1_np_pois(n1, N, freq_n1)
# tau1_NP_bin <- tau1_np_bin(n1, N, freq_n1)
# Binomial approximation
tau1_py_binom1 <- m1_state * (n0 / N)^(1 - out_PY$par[2])
lower_py_binom1 <- qbinom(0.025, m1_state, (n0 / N)^(1 - out_PY$par[2]))
upper_py_binom1 <- qbinom(0.975, m1_state, (n0 / N)^(1 - out_PY$par[2]))
kable(data.frame(n = n0, N = N, percentage = percentage0,
m1 = m1_state,
true_tau1 = tau1_true_state,
tau1_py = tau1_PY, CI_PY = paste("[", PY1_lower,", ", PY1_upper,"]",sep=""),
tau1_py_binapprox = tau1_py_binom1,
CI_PY_binapprox = paste("[", lower_py_binom1,", ", upper_py_binom1,"]",sep=""),
tau1_dp = tau1_DP, CI_DP = paste("[", DP1_lower,", ", DP1_upper,"]",sep="")))
234839-234104
4904-3518
tablestate <- prova2sub %>% group_by(STATEICP, HHINCOME, VALUEH, FAMSIZE, NCHILD, SEX, AGE, MARST,
HCOVANY, EDUC, SCHLTYPE, EMPSTAT,
INCTOT, FTOTINC, VETSTAT) %>% summarize(count=n())
freq_state <- tablestate$count
m_pl = displ$new(freq_state)
est = estimate_xmin(m_pl)
m_pl$setXmin(est)
plot(m_pl, main="US 2018 state")
lines(m_pl, col=2, main="US 2018 state")
tablestate <- prova2sub %>% group_by(STATEICP, HHINCOME, VALUEH, FAMSIZE, NCHILD, SEX, AGE, MARST,
HCOVANY, EDUC, SCHLTYPE, EMPSTAT, OCC,
INCTOT, VETSTAT) %>% summarize(count=n())
freq_state <- tablestate$count
m_pl = displ$new(freq_state)
est = estimate_xmin(m_pl)
m_pl$setXmin(est)
plot(m_pl, main="US 2018 state")
lines(m_pl, col=2, main="US 2018 state")
id_state <- prova2sub %>% add_count(STATEICP, HHINCOME, VALUEH, FAMSIZE, NCHILD, SEX, AGE, MARST,
HCOVANY, EDUC, SCHLTYPE, EMPSTAT, OCC,
INCTOT) %>% filter(n==1) %>% select(id)
id_state <- id_state$id
length(id_state)
sum(freq_state==2)
log(sum(freq_state==2)/N)
log(sum(freq_state==1)/N)
sum(freq_state==1)/N
sum(freq_state==1)/N
sum(freq_state==2)/N
n_id <- as.integer(length(id_state)/2)
id_state_halp <- id_state[1:n_id]
n_id
length(id_state)
id_state_half <- id_state[1:n_id]
id_1 <- freq_state==1
id_1_half <- id_1[1:as.integer(length(id_1)/2)]
freq_state_short <- freq_state[!id_1_half]
as.integer(length(id_1)/2)
1187751+length(freq_state_short)
length(freq_state)
length(id_1)
length(id_1_half)
length(id_1_half)*w
length(id_1_half)*2
freq_state_short <- freq_state[freq_state==1][id_1_half]
length(freq_state_short)+1187751
length(freq_state)
freq_state_1 <- freq_state[freq_state==1][id_1_half]
length(freq_state_1)
length(id_1_half)
freq_state <- sort(tablestate$count)
freq_state_short <- freq_state[id_1_half_length(freq_state)]
m_pl = displ$new(freq_state_short)
est = estimate_xmin(m_pl)
m_pl$setXmin(est)
plot(m_pl, main="US 2018 state")
lines(m_pl, col=2, main="US 2018 state")
freq_state_short <- freq_state[id_1_half_length(freq_state)]
freq_state_short <- freq_state[id_1_half:length(freq_state)]
freq_state_short <- freq_state[as.integer(length(freq_state)/2):length(freq_state)]
m_pl = displ$new(freq_state_short)
est = estimate_xmin(m_pl)
m_pl$setXmin(est)
plot(m_pl, main="US 2018 state")
lines(m_pl, col=2, main="US 2018 state")
freq_state_short <- freq_state[as.integer(length(freq_state)*0.8):length(freq_state)]
m_pl = displ$new(freq_state_short)
est = estimate_xmin(m_pl)
m_pl$setXmin(est)
plot(m_pl, main="US 2018 state")
lines(m_pl, col=2, main="US 2018 state")
id_1 <- length(freq_state==1)
freq_state_short <- freq_state[as.integer(id_1*0.8):length(freq_state)]
m_pl = displ$new(freq_state_short)
est = estimate_xmin(m_pl)
m_pl$setXmin(est)
plot(m_pl, main="US 2018 state")
lines(m_pl, col=2, main="US 2018 state")
id_state_short <- id_state[as.integer(id_1*0.8):length(id_state)]
length(id_state_short)
length(freq_state_short==1)
head(freq_state)
sum(freq_state_short==1)
length(id_state)
sum(freq_state==1)
tablestate <- prova2sub %>% group_by(STATEICP, HHINCOME, VALUEH, FAMSIZE, NCHILD, SEX, AGE, MARST,
HCOVANY, EDUC, SCHLTYPE, EMPSTAT, OCC,
INCTOT, VETSTAT) %>% count()
freq_state <- sort(tablestate$n)
id_1 <- length(freq_state==1)
freq_state_short <- freq_state[as.integer(id_1*0.8):length(freq_state)]
id_state_short <- id_state[as.integer(id_1*0.8):length(id_state)]
length(id_state_short)
sum(freq_state_short==1)
sum(freq_state==1)
length(id_state)
id_state <- prova2sub %>% add_count(STATEICP, HHINCOME, VALUEH, FAMSIZE, NCHILD, SEX, AGE, MARST,
HCOVANY, EDUC, SCHLTYPE, EMPSTAT, OCC,
INCTOT, VETSTAT) %>% filter(n==1) %>% select(id)
id_state <- id_state$id
id_state_short <- id_state[as.integer(id_1*0.8):length(id_state)]
length(id_state_short)
sum(freq_state_short==1)
id_uniqsamp1_state <- sample_n0 %>% add_count(STATEICP, HHINCOME, VALUEH, FAMSIZE, NCHILD, SEX, AGE, MARST,
HCOVANY, EDUC, SCHLTYPE, EMPSTAT, OCC,
INCTOT, VETSTAT) %>%
filter(n==1) %>% select(id)
id_uniqsamp1_state <- id_uniqsamp1_state$id
table_samp_state <- sample_n0 %>% group_by(STATEICP, HHINCOME, VALUEH, FAMSIZE, NCHILD, SEX, AGE, MARST,
HCOVANY, EDUC, SCHLTYPE, EMPSTAT, OCC,
INCTOT, VETSTAT) %>% count()
freq_samp_state <- table_samp_state$n
# compute tau_1 true
id_alluniq_state <- intersect(id_uniqsamp1_state, id_state_short)
tau1_true_state <- length(id_alluniq_state)
# number of uniques in sample
m1_state <- sum(freq_samp_state==1)
tablestate <- prova2sub %>% group_by(STATEICP, HHINCOME, VALUEH, FAMSIZE, NCHILD, SEX, AGE, MARST,
HCOVANY, EDUC, SCHLTYPE, EMPSTAT, OCC, VETSTAT) %>% count()
freq_state <- sort(tablestate$n)
# id_1 <- length(freq_state==1)
# freq_state_short <- freq_state[as.integer(id_1*0.8):length(freq_state)]
m_pl = displ$new(freq_state_short)
est = estimate_xmin(m_pl)
m_pl$setXmin(est)
plot(m_pl, main="US 2018 state")
lines(m_pl, col=2, main="US 2018 state")
# id_1 <- length(freq_state==1)
# freq_state_short <- freq_state[as.integer(id_1*0.8):length(freq_state)]
m_pl = displ$new(freq_state)
est = estimate_xmin(m_pl)
m_pl$setXmin(est)
plot(m_pl, main="US 2018 state")
lines(m_pl, col=2, main="US 2018 state")
HCOVANY, EDUC, SCHLTYPE, EMPSTAT, OCC, VETSTAT) %>% count()
freq_state <- sort(tablestate$n)
# id_1 <- length(freq_state==1)
# freq_state_short <- freq_state[as.integer(id_1*0.8):length(freq_state)]
m_pl = displ$new(freq_state)
est = estimate_xmin(m_pl)
m_pl$setXmin(est)
plot(m_pl, main="US 2018 state")
lines(m_pl, col=2, main="US 2018 state")
tablestate <- prova2sub %>% group_by(STATEICP, HHINCOME, VALUEH, FAMSIZE, NCHILD, SEX, AGE, MARST, RACE,
HCOVANY, EDUC, SCHLTYPE, EMPSTAT, OCC, VETSTAT) %>% count()
freq_state <- sort(tablestate$n)
# id_1 <- length(freq_state==1)
# freq_state_short <- freq_state[as.integer(id_1*0.8):length(freq_state)]
m_pl = displ$new(freq_state)
est = estimate_xmin(m_pl)
m_pl$setXmin(est)
plot(m_pl, main="US 2018 state")
lines(m_pl, col=2, main="US 2018 state")
colnames(prova2sub)
tablestate <- prova2sub %>% group_by(REGION, HHINCOME, VALUEH, FAMSIZE, NCHILD, SEX, AGE, MARST,
HCOVANY, EDUC, SCHLTYPE, EMPSTAT, OCC,
INCTOT, FTOTINC, VETSTAT) %>% count()
freq_state <- sort(tablestate$n)
# id_1 <- length(freq_state==1)
# freq_state_short <- freq_state[as.integer(id_1*0.8):length(freq_state)]
m_pl = displ$new(freq_state)
est = estimate_xmin(m_pl)
m_pl$setXmin(est)
plot(m_pl, main="US 2018 state")
lines(m_pl, col=2, main="US 2018 state")
id_state <- prova2sub %>% add_count(REGION, HHINCOME, VALUEH, FAMSIZE, NCHILD, SEX, AGE, MARST,
HCOVANY, EDUC, SCHLTYPE, EMPSTAT, OCC,
INCTOT, FTOTINC, VETSTAT) %>% filter(n==1) %>% select(id)
id_state <- id_state$id
N = dim(prova2sub)[1]
percentage0 = 0.1
n0 = as.integer(percentage0*N)
ind0 <- sample(1:N, n0, replace = FALSE)
sample_n0 <- prova2sub[ind0,]
id_uniqsamp1_state <- sample_n0 %>% add_count(REGION, HHINCOME, VALUEH, FAMSIZE, NCHILD, SEX, AGE, MARST,
HCOVANY, EDUC, SCHLTYPE, EMPSTAT, OCC,
INCTOT, FTOTINC, VETSTAT) %>%
filter(n==1) %>% select(id)
id_uniqsamp1_state <- id_uniqsamp1_state$id
table_samp_state <- sample_n0 %>% group_by(REGION, HHINCOME, VALUEH, FAMSIZE, NCHILD, SEX, AGE, MARST,
HCOVANY, EDUC, SCHLTYPE, EMPSTAT, OCC,
INCTOT, FTOTINC, VETSTAT) %>% count()
freq_samp_state <- table_samp_state$n
# compute tau_1 true
id_alluniq_state <- intersect(id_uniqsamp1_state, id_state)
tau1_true_state <- length(id_alluniq_state)
# comparison uniques in sample AND pop, uniques in sample, uniques in population
tau1_true_state # pop and sample uniques
length(id_state)/N # pop uniques
length(id_uniqsamp1_state)/n0 # sample uniques
# number of uniques in sample
m1_state <- sum(freq_samp_state==1)
# # if you wanna plot the frequency counts
# tablefreq_state <- table(freq_samp_state)
# tablefreq_state <- data.frame("frequency"=as.factor(names(tablefreq_state)), "count"=as.numeric(tablefreq_state))
# ggplot(data=tablefreq_state, aes(reorder(frequency, -count), log(count))) + geom_col() + ggtitle('City log frequencies counts sample 10%')
# Stime
out_PY <- max_EPPF_PY(freq_samp_state)
tau1_PY <- tau1_py(m1_state, n0, out_PY$par[1], out_PY$par[2], N)
PY1_sim <- tau1_py_sim(freq_samp_state, out_PY$par[1], out_PY$par[2], N)
PY1_lower <- quantile(PY1_sim, 0.025)
PY1_upper <- quantile(PY1_sim, 0.975)
out_DP <- max_EPPF_DP(freq_samp_state)
tau1_DP <- tau1_dp(m1_state, n0, out_DP$par, N)
DP1_sim  <- tau1_py_sim(freq_samp_state, out_DP$par, 0, N)
DP1_lower <- quantile(DP1_sim, 0.025)
DP1_upper <- quantile(DP1_sim, 0.975)
# tau1_NP_pois <- tau1_np_pois(n1, N, freq_n1)
# tau1_NP_bin <- tau1_np_bin(n1, N, freq_n1)
# Binomial approximation
tau1_py_binom1 <- m1_state * (n0 / N)^(1 - out_PY$par[2])
lower_py_binom1 <- qbinom(0.025, m1_state, (n0 / N)^(1 - out_PY$par[2]))
upper_py_binom1 <- qbinom(0.975, m1_state, (n0 / N)^(1 - out_PY$par[2]))
kable(data.frame(n = n0, N = N, percentage = percentage0,
m1 = m1_state,
true_tau1 = tau1_true_state,
tau1_py = tau1_PY, CI_PY = paste("[", PY1_lower,", ", PY1_upper,"]",sep=""),
tau1_py_binapprox = tau1_py_binom1,
CI_PY_binapprox = paste("[", lower_py_binom1,", ", upper_py_binom1,"]",sep=""),
tau1_dp = tau1_DP, CI_DP = paste("[", DP1_lower,", ", DP1_upper,"]",sep="")))
kable(data.frame(n = n0, N = N, percentage = percentage0,
m1 = m1_state,
true_tau1 = tau1_true_state,
K_n = dim(table_samp_state)[1],
tau1_py = tau1_PY, CI_PY = paste("[", PY1_lower,", ", PY1_upper,"]",sep=""),
tau1_py_binapprox = tau1_py_binom1,
CI_PY_binapprox = paste("[", lower_py_binom1,", ", upper_py_binom1,"]",sep=""),
tau1_dp = tau1_DP, CI_DP = paste("[", DP1_lower,", ", DP1_upper,"]",sep="")))
plot(m_pl, main="US 2018 state")
m_pl = displ$new(freq_samp_state)
est = estimate_xmin(m_pl)
m_pl$setXmin(est)
plot(m_pl, main="US 2018 state SAMPLE")
